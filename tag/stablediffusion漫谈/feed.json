{
    "version": "https://jsonfeed.org/version/1",
    "title": "明燊小站 • All posts by \"stablediffusion漫谈\" tag",
    "description": "",
    "home_page_url": "http://example.com",
    "items": [
        {
            "id": "http://example.com/2023/04/01/%E6%BC%AB%E8%B0%88Stable-Diffusion/",
            "url": "http://example.com/2023/04/01/%E6%BC%AB%E8%B0%88Stable-Diffusion/",
            "title": "漫谈Stable Diffusion-第一章",
            "date_published": "2023-04-01T08:05:44.000Z",
            "content_html": "<h1 id=\"漫谈stable-diffusion-第一章\"><a class=\"anchor\" href=\"#漫谈stable-diffusion-第一章\">#</a> 漫谈 Stable Diffusion - 第一章</h1>\n<h2 id=\"前言\"><a class=\"anchor\" href=\"#前言\">#</a> 前言</h2>\n<p>这篇文章主要是谈一下目前我对 Stable Diffusion（以下简称 SD）的一些认识和一些简单的 SD 入门知识。</p>\n<h2 id=\"了解stable-diffusion\"><a class=\"anchor\" href=\"#了解stable-diffusion\">#</a> 了解 Stable Diffusion</h2>\n<h3 id=\"什么是stable-diffusion\"><a class=\"anchor\" href=\"#什么是stable-diffusion\">#</a> 什么是 Stable Diffusion</h3>\n<p>Stable Diffusion 是由 Stability AI 开发并开源的 AI 绘画扩散模型，Stability 官方提供收费的线上版本 DreanStudio 和 Reimagine 服务。（简单来说，DreamStudio 相当于用他们的 GPU 算图，Reimagine 相当于加强版的 img2img）</p>\n<p><img data-src=\"https://s2.loli.net/2023/04/01/MVrvTPyJE5hYKoS.png\" alt=\"Picsee-20230401174425\" /></p>\n<h3 id=\"stable-diffusion-的优势是什么\"><a class=\"anchor\" href=\"#stable-diffusion-的优势是什么\">#</a> Stable Diffusion 的优势是什么</h3>\n<h4 id=\"开源社区的维护\"><a class=\"anchor\" href=\"#开源社区的维护\">#</a> 开源社区的维护</h4>\n<p>作为开源的扩散模型，Stable Diffusion 有庞大的维护者团体，有时候更新频率甚至以分钟来计数（不像 OpenAI 嘴上说着 Open 却一直想着搞钱）</p>\n<p><strong>（最近 Github 上 SD WebUI 的维护者正在更新 Gradio 库，所以 WebUI 开发版本几乎不能用，可能要等几周后插件和后端适配才行）</strong></p>\n<h4 id=\"高度的可拓展性\"><a class=\"anchor\" href=\"#高度的可拓展性\">#</a> 高度的可拓展性</h4>\n<p>正是由于开源的特性，很多新技术得以在 SD 上实现，比如 LoRA 和 Control Net 都是其他工具不具备的。</p>\n<p><img data-src=\"https://s2.loli.net/2023/04/01/tLZHowATFRfaOP4.png\" alt=\"Picsee-20230401175931\" /></p>\n<h4 id=\"多平台的兼容性以及较低的性能要求\"><a class=\"anchor\" href=\"#多平台的兼容性以及较低的性能要求\">#</a> 多平台的兼容性以及较低的性能要求</h4>\n<p>SD 的 Webui 版本支持几乎所有平台，Windows, macOS, GNU/Linux 均有相应的启动脚本。</p>\n<p>我所使用的设备是 Apple m1 MacBook Pro 16g，绕过 CUDA 使用 CPU（其实就是 m1 核显） 计算一张 512*512 的图片仅要四十秒左右。如果显存高并且有 CUDA 的话（以 3090 为例），甚至可以一秒钟出一张。如果不炼模型的话一张性能稍高的游戏显卡可以满足所有需求，炼丹的话 A100 一张完全没问题。商业化成本算是很低。</p>\n<p>* 另：MidJourney 似乎也是用的魔改 SD。</p>\n<h2 id=\"简单了解-stable-diffusion-模型\"><a class=\"anchor\" href=\"#简单了解-stable-diffusion-模型\">#</a> 简单了解 Stable Diffusion 模型</h2>\n<h3 id=\"章节前言\"><a class=\"anchor\" href=\"#章节前言\">#</a> 章节前言</h3>\n<p>由于目前使用 AI 绘画的群体大多数都用其来生成动漫风格的图片，所以本章节演示的图片均为动漫风格图片，实物风同理。</p>\n<h3 id=\"模型种类\"><a class=\"anchor\" href=\"#模型种类\">#</a> 模型种类</h3>\n<h4 id=\"checkpoint\"><a class=\"anchor\" href=\"#checkpoint\">#</a> CheckPoint</h4>\n<p>Checkpoint，通常后缀为 ckpt 或者 safetensors，是 AI 绘画的基础大模型。有 checkpoint 模型就可以直接生成画面，其他种类的模型可以理解为辅助 CheckPoint</p>\n<h4 id=\"lora\"><a class=\"anchor\" href=\"#lora\">#</a> LoRA</h4>\n<p>LoRA（Low-Rank Adaptation of Large Language Models）简单理解就是对大模型的内容进行微调（对于 AI 绘画来说调整很大）</p>\n<blockquote>\n<p>比如，GPT-3 有 1750 亿参数，为了让它能干特定领域的活儿，需要做微调，但是如果直接对 GPT-3 做微调，成本太高太麻烦了。</p>\n<p>LoRA 的做法是，冻结预训练好的模型权重参数，然后在每个 Transformer（Transformer 就是 GPT 的那个 T）块里注入可训练的层，由于不需要对模型的权重参数重新计算梯度，所以，大大减少了需要训练的计算量。</p>\n<p>研究发现，LoRA 的微调质量与全模型微调相当，我愿称之为神器。</p>\n<p>要做个比喻的话，就好比是大模型的一个小模型，或者说是一个插件。</p>\n<p>LoRA 本来是给大语言模型准备的，但把它用在 cross-attention layers（交叉关注层）也能影响用文字生成图片的效果。</p>\n<p>── 知乎用户 奉孝翼德 <span class=\"exturl\" data-url=\"aHR0cHM6Ly96aHVhbmxhbi56aGlodS5jb20vcC82MTAwMzE3MTM=\">Stable Diffusion 爱好者常说的 LoRa 是什么？</span></p>\n</blockquote>\n<h4 id=\"vae\"><a class=\"anchor\" href=\"#vae\">#</a> Vae</h4>\n<p>Vae 涉及的原理很复杂，在此不赘述。只要不加载 Vae 会使画面发灰就行了</p>\n<h3 id=\"针对于模型样例的对比示例为控制变量以下图片-seed采样方式采样步数均相同无后处理\"><a class=\"anchor\" href=\"#针对于模型样例的对比示例为控制变量以下图片-seed采样方式采样步数均相同无后处理\">#</a> 针对于模型样例的对比示例（为控制变量，以下图片 Seed，采样方式，采样步数均相同，无后处理）</h3>\n<p>Anything V3.0 Checkpoint，不加载 LoRA</p>\n<p><img data-src=\"https://s2.loli.net/2023/04/01/QNmSEcC4dLuODos.png\" alt=\"00010-3405425322\" /></p>\n<p>Anything V3.0 Checkpoint，加载 chibi LoRA</p>\n<p><img data-src=\"https://s2.loli.net/2023/04/01/sxzlkPACHgbcUaQ.png\" alt=\"00013-3405425322\" /></p>\n<p>Anything V3.0 Checkpoint，加载 Nahida LoRA</p>\n<p><img data-src=\"https://s2.loli.net/2023/04/01/fuRtNcDlvW8Pkxi.png\" alt=\"00015-3405425322\" /></p>\n<p>Pastel-Mix Checkpoint，不加载 LoRA</p>\n<p><img data-src=\"https://s2.loli.net/2023/04/01/UnTbIXHc29hewCS.png\" alt=\"00011-3405425322\" /></p>\n<p>由此可以看出不同的模型对画面的影响</p>\n<h2 id=\"商业化前景\"><a class=\"anchor\" href=\"#商业化前景\">#</a> 商业化前景</h2>\n<p>现在写实风格的 Checkpoint 最主要的还是 sd 模型，可以到抱脸站（Hugging Face）或者其他模型网站下载。</p>\n<p>AI 绘图通过炼制模型低成本地生成基本图片，经过轻度人工润色，以实现商业化。（亦可作为设计师的灵感来源。）</p>\n<p>目前比较低成本的方式是通过租用云 GPU 服务器（几块钱一小时）训练自己的 LoRA（训练量小，需求的图片训练集小，速度快，配置要求低）。非常不推荐训练 Checkpoint（ckpt 一般通过合并多个模型来实现炼制）。通过自己的 LoRA 结合 sd 模型来达到需求</p>\n<p>已有人成功过，详见</p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMVpYNHkxVTdXeS8/c3BtX2lkX2Zyb209MzMzLjMzNy5zZWFyY2gtY2FyZC5hbGwuY2xpY2s=\">AI Lora 技术，跑通炼实物模型 + 写实风格，商业化还差半步～</span></p>\n<p><span class=\"exturl\" data-url=\"aHR0cHM6Ly93d3cuYmlsaWJpbGkuY29tL3ZpZGVvL0JWMUprNHkxYjdtcy8/c3BtX2lkX2Zyb209MzMzLjc4OC50b3BfcmlnaHRfYmFyX3dpbmRvd19oaXN0b3J5LmNvbnRlbnQuY2xpY2smYW1wO3ZkX3NvdXJjZT01MmI4ZThjZDBlZjA0OGVkMDdmNzU1ZDE3YzY3NzRiNw==\">摩飞（Morphy Richards）生活家电产品风格 lora 训练效果</span></p>\n<h2 id=\"补充说明\"><a class=\"anchor\" href=\"#补充说明\">#</a> 补充说明</h2>\n<p>本专栏还会有第二期，会讲解 WebUI 各项功能的用法和具体介绍 ControlNet 的用法，敬请期待。</p>\n<p><strong>跑图的话 4g 左右显存 N 卡就够了，训练的话要 8g 显存的 n 卡起步（至少 RTX 3070+）云服务器方式我没试过，但是很多人炼丹会选用</strong></p>\n",
            "tags": [
                "StableDiffusion漫谈"
            ]
        }
    ]
}